{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import serial\n",
    "from threading import Thread\n",
    "import time\n",
    "import re\n",
    "\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib qt\n",
    "\n",
    "import cv2\n",
    "import os.path\n",
    "import numpy as np\n",
    "from utils.eval import *\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the Serial\n",
    "ser = serial.Serial('COM3', 9600)\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_serial(serial_msg):\n",
    "    \"\"\"\n",
    "    Function to parse serial data to extract float values\n",
    "    format 'x:19.34 y:23.01 z:-33.83' to x, y, z float values\n",
    "\n",
    "    Args:\n",
    "        - (str) string with format 'x:19.34 y:23.01 z:-33.83'\n",
    "    Return:\n",
    "        - (list) x, y, z float values\n",
    "    \"\"\"\n",
    "    xyz_list = re.findall('[-+]?[0-9]*\\.?[0-9]*', serial_msg)\n",
    "    return [float(i) for i in filter(lambda item: item, xyz_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UpdateOrientation(ser):\n",
    "    \"\"\"\n",
    "    Function to integration the x data from the Gyroscope\n",
    "    and update the global variable x_orientation with the new value\n",
    "\n",
    "    Args:\n",
    "        - (serial.Serial) serial to get the gyroscope data\n",
    "    \"\"\"\n",
    "    global x_orientation\n",
    "\n",
    "    while True:\n",
    "        serial_msg_bytes = ser.readline()\n",
    "        serial_msg = serial_msg_bytes.decode()\n",
    "        dx, dy, dz = parse_serial(serial_msg)\n",
    "        \n",
    "        # The gyroscope values are in degrees-per-second\n",
    "        # divide each value by the number of samples per second\n",
    "        dx_normalized = dx / gyroscope_sample_rate;\n",
    "\n",
    "        # remove noise\n",
    "        if(abs(dx_normalized) > 0.004):\n",
    "            # update orientation\n",
    "            x_orientation = x_orientation - dx_normalized*1.25\n",
    "            x_orientation = x_orientation%360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rotation_matrix(orientation):\n",
    "    \"\"\"\n",
    "    Calculate the rotation matrix for a rotation around the x axis of n radians\n",
    "\n",
    "    Args:\n",
    "        - (float) orientation in radian\n",
    "    Return:\n",
    "        - (np.array) rotation matrix for a rotation around the x axis\n",
    "    \"\"\"\n",
    "    rotation_matrix = np.array([[1, 0, 0],\n",
    "                                [0, math.cos(orientation), -math.sin(orientation)],\n",
    "                                [0, math.sin(orientation), math.cos(orientation)]])\n",
    "    return rotation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update orientation in real time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize x orientation\n",
    "x_orientation = 0\n",
    "\n",
    "# 119 got from Arduino with IMU.gyroscopeSampleRate();\n",
    "gyroscope_sample_rate = 119"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the thread to update the x orientation in real time\n",
    "Thread(target=UpdateOrientation, args=(ser,)).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot orientation in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    plt.clf()\n",
    "\n",
    "    # plot origin in blue\n",
    "    plt.scatter(0, 0, s=100, c='b')\n",
    "\n",
    "    # find position of the end of the needle\n",
    "    # 0.1 far from origin in direction of the orientation\n",
    "    distance = 0.1\n",
    "    x_orientation_rad = math.radians(x_orientation)\n",
    "    x_pos = math.cos(x_orientation_rad)*distance\n",
    "    y_pos = math.sin(x_orientation_rad)*distance\n",
    "\n",
    "    # plot line between both position with circles\n",
    "    plt.plot([0, x_pos], [0, y_pos], 'ro-')\n",
    "    \n",
    "    plt.xlim([-distance, distance])\n",
    "    plt.ylim([-distance, distance])\n",
    "    plt.pause(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot orientation in 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "while True:\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # plot origin in blue\n",
    "    ax.scatter(0, 0, s=100, c='b')\n",
    "\n",
    "    # find position of the end of the needle\n",
    "    # 0.1 far from origin in direction of the orientation\n",
    "    distance = 0.1\n",
    "    x_orientation_rad = math.radians(x_orientation)\n",
    "    x_pos = math.cos(x_orientation_rad)*distance\n",
    "    y_pos = math.sin(x_orientation_rad)*distance\n",
    "\n",
    "    # plot line between both position with circles\n",
    "    ax.quiver(0, 0, 0, x_pos, y_pos, 0,\n",
    "              length=distance, normalize=True)\n",
    "\n",
    "    ax.set_xlim(-distance, distance)\n",
    "    ax.set_ylim(-distance, distance)\n",
    "    ax.set_zlim(-distance, distance)\n",
    "   \n",
    "    plt.show()\n",
    "    plt.pause(0.1)\n",
    "    plt.gcf().clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get depth image and do projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFLITE_FILE_PATH = 'model/midas_v2_1_small.tflite'\n",
    "\n",
    "if not os.path.isfile(TFLITE_FILE_PATH):\n",
    "    tflite_model_url = \"https://tfhub.dev/intel/lite-model/midas/v2_1_small/1/lite/1?lite-format=tflite\"\n",
    "    urllib.request.urlretrieve(tflite_model_url, TFLITE_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=TFLITE_FILE_PATH)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x15f5a1e9b80>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get input image\n",
    "img = cv2.imread('images/dog.jpg')\n",
    "rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# preprocess input image\n",
    "input_data = preprocess_image(rgb_img, [256, 256])\n",
    "\n",
    "# reshape data according to input_details\n",
    "input_data = tf.transpose(input_data, [0, 2, 3, 1])\n",
    "\n",
    "# Get result\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "interpreter.invoke()\n",
    "output_data = tf.squeeze(interpreter.get_tensor(output_details[0]['index']), axis=0)\n",
    "\n",
    "depth_map = output_data.numpy()\n",
    "plt.imshow(depth_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=180.79759>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=883.7121>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_min(output_data), tf.reduce_max(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_depth_value = tf.reduce_min(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_vector = np.array([0.1, 0.1, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1       , 0.13660254, 0.03660254])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(unit_vector, get_rotation_matrix(math.radians(30)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1280/1280 [00:00<00:00, 1641.04it/s]\n"
     ]
    }
   ],
   "source": [
    "image_width = 1280\n",
    "image_height = 720\n",
    "\n",
    "x_depth_rescale_factor = depth_map.shape[0] / image_width\n",
    "y_depth_rescale_factor = depth_map.shape[1] / image_height\n",
    "\n",
    "h_fov = math.radians(60)\n",
    "v_fov = math.radians(35)\n",
    "\n",
    "x_focal = image_width / (2*math.tan(h_fov/2))\n",
    "y_focal = image_height / (2*math.tan(v_fov/2))\n",
    "\n",
    "c_x = (0.5*image_width)\n",
    "c_y = (0.5*image_height)\n",
    "\n",
    "points_in_ned = []\n",
    "depth_values = []\n",
    "\n",
    "for x in tqdm(range(image_width)):\n",
    "    for y in range(image_height):\n",
    "\n",
    "        # keep 0.1% of the points\n",
    "        if random.randint(0, 999) >= 1:\n",
    "            continue\n",
    "\n",
    "        # get depth value\n",
    "        x_depth_pos = int(x*x_depth_rescale_factor)\n",
    "        y_depth_pos = int(y*y_depth_rescale_factor)\n",
    "        depth_value = depth_map[x_depth_pos, y_depth_pos, 0]\n",
    "\n",
    "        # get 3d vector\n",
    "        x_point = depth_value * (x - c_x) / x_focal\n",
    "        y_point = depth_value * (y - c_y) / y_focal\n",
    "        point_3d_before_rotation = np.array([x_point, y_point, depth_value])\n",
    "\n",
    "        # projection in function of the orientation\n",
    "        point_3d_after_rotation = np.matmul(get_rotation_matrix(math.radians(x_orientation)), point_3d_before_rotation)\n",
    "        points_in_ned.append(point_3d_after_rotation)\n",
    "        depth_values.append(depth_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "877"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_in_ned = np.array(points_in_ned)\n",
    "len(points_in_ned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "rainbow = cm.get_cmap('rainbow', 12)\n",
    "depth_values_normalized = depth_values/max(depth_values)\n",
    "colormap = rainbow(depth_values_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# plot origin in blue\n",
    "ax.scatter(0, 0, s=100, c='b')\n",
    "\n",
    "# plot x, y, z referential\n",
    "ax.quiver(0, 0, 0, 1, 0, 0,\n",
    "          length=min_depth_value/2, normalize=True, color='r')\n",
    "ax.text(min_depth_value/2, 0, 0, \"x\", color='r')\n",
    "ax.quiver(0, 0, 0, 0, 1, 0,\n",
    "          length=min_depth_value/2, normalize=True, color='g')\n",
    "ax.text(0, min_depth_value/2, 0, \"y\", color='g')\n",
    "ax.quiver(0, 0, 0, 0, 0, 1,\n",
    "          length=min_depth_value/2, normalize=True, color='b')\n",
    "ax.text(0, 0, min_depth_value/2, \"z\", color='b')\n",
    "\n",
    "# find position of the end of the needle\n",
    "x_orientation_rad = math.radians(x_orientation)\n",
    "x_pos = 0\n",
    "y_pos = -math.sin(x_orientation_rad)\n",
    "z_pos = math.cos(x_orientation_rad)\n",
    "\n",
    "# plot arrow for robot orientation\n",
    "ax.quiver(0, 0, 0, -z_pos, y_pos, x_pos,\n",
    "          length=min_depth_value/2, normalize=True, color='black')\n",
    "\n",
    "# plot projected points in 3D\n",
    "ax.scatter(-points_in_ned[:, 2], points_in_ned[:, 1], points_in_ned[:, 0], c=colormap)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calcul projection in real time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFLITE_FILE_PATH = 'model/midas_v2_1_small.tflite'\n",
    "\n",
    "if not os.path.isfile(TFLITE_FILE_PATH):\n",
    "    tflite_model_url = \"https://tfhub.dev/intel/lite-model/midas/v2_1_small/1/lite/1?lite-format=tflite\"\n",
    "    urllib.request.urlretrieve(tflite_model_url, TFLITE_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=TFLITE_FILE_PATH)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_width, depth_height = output_details[0]['shape'][1], output_details[0]['shape'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_width = 1280\n",
    "image_height = 720\n",
    "\n",
    "x_depth_rescale_factor = depth_width / image_width\n",
    "y_depth_rescale_factor = depth_height / image_height\n",
    "\n",
    "h_fov = math.radians(60)\n",
    "# v_fov is wrong but cannot find the real value on camera's documentation\n",
    "v_fov = math.radians(image_height/image_width*60)\n",
    "\n",
    "x_focal = image_width / (2*math.tan(h_fov/2))\n",
    "y_focal = image_height / (2*math.tan(v_fov/2))\n",
    "\n",
    "c_x = (0.5*image_width)\n",
    "c_y = (0.5*image_height)\n",
    "\n",
    "rainbow = cm.get_cmap('rainbow', 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5773502691896257, 0.3033466836073424)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(image_width - c_x) / x_focal, (image_height - c_y) / y_focal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.lite.python.interpreter.Interpreter"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(interpreter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_depthmap(interpreter, rgb_img):\n",
    "    \"\"\"\n",
    "    Infer model on an image\n",
    "\n",
    "    Args:\n",
    "        - (tf.lite.python.interpreter.Interpreter) tf lite interpreter\n",
    "        - (cv2 image) image in rgb format\n",
    "    Return:\n",
    "        - (np.array) depthmap in format (width, height, 1)\n",
    "    \"\"\"\n",
    "    # preprocess input image\n",
    "    input_data = preprocess_image(rgb_img, [256, 256])\n",
    "\n",
    "    # reshape data according to input_details\n",
    "    input_data = tf.transpose(input_data, [0, 2, 3, 1])\n",
    "\n",
    "    # Get result\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    interpreter.invoke()\n",
    "    output_data = tf.squeeze(interpreter.get_tensor(output_details[0]['index']), axis=0)\n",
    "\n",
    "    depth_map = output_data.numpy()\n",
    "\n",
    "    return depth_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_3d_points_from_depthmap(points_in_ned, depth_values,\n",
    "                                depth_map, pourcentage_to_keep=1):\n",
    "    \"\"\"\n",
    "    Project depth values into 3D point according to the robot orientation\n",
    "    Uses global variable x_orientation\n",
    "\n",
    "    Args:\n",
    "        - (np.array) points_in_ned\n",
    "        - (list) depth_values\n",
    "        - (cv2 image) depth_map format (width, height, 1)\n",
    "        - (int) pourcentage_to_keep: pourcentage of depth points to project\n",
    "    Return:\n",
    "        - (np.array) rotation matrix for a rotation around the x axis\n",
    "    \"\"\"\n",
    "    for x in tqdm(range(image_width)):\n",
    "        for y in range(image_height):\n",
    "\n",
    "            # keep 0.1% of the points\n",
    "            if random.randint(0, 999) >= pourcentage_to_keep:\n",
    "                continue\n",
    "\n",
    "            # get depth value\n",
    "            x_depth_pos = int(x*x_depth_rescale_factor)\n",
    "            y_depth_pos = int(y*y_depth_rescale_factor)\n",
    "            depth_value = depth_map[x_depth_pos, y_depth_pos, 0]\n",
    "\n",
    "            # get 3d vector\n",
    "            x_point = depth_value * (x - c_x) / x_focal\n",
    "            y_point = depth_value * (y - c_y) / y_focal\n",
    "            point_3d_before_rotation = np.array([x_point, y_point, depth_value])\n",
    "\n",
    "            # projection in function of the orientation\n",
    "            point_3d_after_rotation = np.matmul(get_rotation_matrix(math.radians(x_orientation)), point_3d_before_rotation)\n",
    "            points_in_ned = np.append(points_in_ned, point_3d_after_rotation)\n",
    "            depth_values.append(depth_value)\n",
    "    return points_in_ned, depth_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1280/1280 [00:00<00:00, 1546.51it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1280/1280 [00:00<00:00, 1478.38it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1280/1280 [00:00<00:00, 1499.64it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1280/1280 [00:00<00:00, 1427.42it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1280/1280 [00:00<00:00, 1429.36it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1280/1280 [00:00<00:00, 1418.68it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1280/1280 [00:00<00:00, 1355.70it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1280/1280 [00:00<00:00, 1368.60it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1280/1280 [00:00<00:00, 1337.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get input image\n",
    "img = cv2.imread('images/dog.jpg')\n",
    "rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "points_in_ned = np.array([])\n",
    "depth_values = []\n",
    "\n",
    "x_orientation = 0\n",
    "\n",
    "while True:\n",
    "    x_orientation += 40\n",
    "    if x_orientation > 360:\n",
    "        break\n",
    "    else:\n",
    "        plt.gcf().clear()\n",
    "\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # get 3d points in real referential\n",
    "    depth_map = get_depthmap(interpreter, rgb_img)\n",
    "    points_in_ned, depth_values = get_3d_points_from_depthmap(points_in_ned, depth_values, depth_map, pourcentage_to_keep=5)\n",
    "\n",
    "    # get colormap\n",
    "    max_depth_value = max(depth_values)\n",
    "    min_depth_value = min(depth_values)\n",
    "    depth_values_normalized = depth_values/max_depth_value\n",
    "    colormap = rainbow(depth_values_normalized)\n",
    "\n",
    "    # plot origin as blue sphere\n",
    "    ax.scatter(0, 0, s=100, c='b')\n",
    "\n",
    "    # plot x, y, z referential with arrows\n",
    "    ax.quiver(0, 0, 0, 1, 0, 0,\n",
    "              length=min_depth_value/2, normalize=True, color='r')\n",
    "    ax.text(min_depth_value/2, 0, 0, \"x\", color='r')\n",
    "    ax.quiver(0, 0, 0, 0, 1, 0,\n",
    "              length=min_depth_value/2, normalize=True, color='g')\n",
    "    ax.text(0, min_depth_value/2, 0, \"y\", color='g')\n",
    "    ax.quiver(0, 0, 0, 0, 0, 1,\n",
    "              length=min_depth_value/2, normalize=True, color='b')\n",
    "    ax.text(0, 0, min_depth_value/2, \"z\", color='b')\n",
    "\n",
    "    # get robot orientation in real referential\n",
    "    x_orientation_rad = math.radians(x_orientation)\n",
    "    x_pos = 0\n",
    "    y_pos = -math.sin(x_orientation_rad)\n",
    "    z_pos = math.cos(x_orientation_rad)\n",
    "\n",
    "    # plot arrow for robot orientation in simulation referential (-z, y, x)\n",
    "    ax.quiver(0, 0, 0, -z_pos, y_pos, x_pos,\n",
    "              length=min_depth_value*0.7, normalize=True, color='black')\n",
    "\n",
    "    # plot 3D projected points in  simulation referential (-z, y, x)\n",
    "    points_in_ned = points_in_ned.reshape([-1, 3])\n",
    "    ax.scatter(-points_in_ned[:, 2], points_in_ned[:, 1], points_in_ned[:, 0], c=colormap, s=5)\n",
    "\n",
    "    ax.view_init(elev=30, azim=10)\n",
    "    ax.set_xlim(-max_depth_value*0.7, max_depth_value*0.7)\n",
    "    ax.set_ylim(-max_depth_value*0.7, max_depth_value*0.7)\n",
    "    ax.set_zlim(-max_depth_value*0.7, max_depth_value*0.7)\n",
    "    plt.show()\n",
    "    plt.pause(0.1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
